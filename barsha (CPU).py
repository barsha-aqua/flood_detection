# -*- coding: utf-8 -*-
"""barsha.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h42TF0uXZ4ITJEB2_mEsjhwewr1Q9erL
"""

#libraries import gareko
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import time

from google.colab import drive
drive.mount('/content/drive')

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay

import xgboost as xgb
sns.set(style="whitegrid")

import xgboost as xgb
sns.set(style="whitegrid")

df = pd.read_csv('/content/drive/MyDrive/mlpc/synthetic_flood_dataset.csv')
print("Shape of dataset:", df.shape)
df.head()

X = df.drop(columns=['FLOOD'])
y = df['FLOOD']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, shuffle=True
)

start_cpu = time.time()

# Fewer estimators to reduce overfitting slightly
rf_model = RandomForestClassifier(n_estimators=60, random_state=42, n_jobs=-1)
rf_model.fit(X_train, y_train)

cpu_train_time = time.time() - start_cpu
y_pred_rf = rf_model.predict(X_test)

print(" Random Forest (cpu)")
print("Accuracy:", accuracy_score(y_test, y_pred_rf))
print("Training Time:", cpu_train_time, "seconds")
print(classification_report(y_test, y_pred_rf))

start_xgb = time.time()

xgb_model = xgb.XGBClassifier(
    tree_method='hist',  # Fast cpu method
    predictor='cpu_predictor',
    use_label_encoder=False,
    eval_metric='logloss',
    max_depth=3,
    n_estimators=70
)

xgb_model.fit(X_train, y_train)
xgb_train_time = time.time() - start_xgb
y_pred_xgb = xgb_model.predict(X_test)

print(" XGBoost (cpu)")
print("Accuracy:", accuracy_score(y_test, y_pred_xgb))
print("Training Time:", xgb_train_time, "seconds")
print(classification_report(y_test, y_pred_xgb))

plt.figure(figsize=(12, 5))

# RF
plt.subplot(1, 2, 1)
ConfusionMatrixDisplay.from_predictions(y_test, y_pred_rf, cmap='Blues', ax=plt.gca())
plt.title("Confusion Matrix: Random Forest (cpu)")

# XGB
plt.subplot(1, 2, 2)
ConfusionMatrixDisplay.from_predictions(y_test, y_pred_xgb, cmap='Greens', ax=plt.gca())
plt.title("Confusion Matrix: XGBoost (cpu)")

plt.tight_layout()
plt.show()

models = ['Random Forest (cpu)', 'XGBoost (cpu)']
accuracies = [accuracy_score(y_test, y_pred_rf), accuracy_score(y_test, y_pred_xgb)]
times = [cpu_train_time, xgb_train_time]

# Accuracy Bar Chart
plt.figure(figsize=(10, 4))
plt.subplot(1, 2, 1)
sns.barplot(x=models, y=accuracies, palette='pastel')
plt.title("Model Accuracy Comparison")
plt.ylabel("Accuracy")

# Training Time Chart
plt.subplot(1, 2, 2)
sns.barplot(x=models, y=times, palette='muted')
plt.title("Training Time Comparison")
plt.ylabel("Seconds")

plt.tight_layout()
plt.show()

important_cols = ['river_level', 'population_density', 'infrastructure_score', 'avg_temperature', 'FLOOD']

for col in important_cols:
    plt.figure(figsize=(6, 4))
    sns.histplot(df[col], kde=True, bins=20)
    plt.title(f"Histogram of {col}")
    plt.grid(True)
    plt.tight_layout()
    plt.show()

# Correlation Heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(df.corr(numeric_only=True), annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title("Feature Correlation Heatmap")
plt.tight_layout()
plt.show()

# Boxplot: River Level vs Flood
plt.figure(figsize=(6, 4))
sns.boxplot(x='FLOOD', y='river_level', data=df, palette='Set2')
plt.title("River Level by Flood Label")
plt.tight_layout()
plt.show()

start_lr = time.time()

# Logistic Regression Model
lr_model = LogisticRegression(max_iter=10000, random_state=40)
lr_model.fit(X_train, y_train)

lr_train_time = time.time() - start_lr
y_pred_lr = lr_model.predict(X_test)

print(" Logistic Regression (cpu)")
print("Accuracy:", accuracy_score(y_test, y_pred_lr))
print("Training Time:", lr_train_time, "seconds")
print(classification_report(y_test, y_pred_lr))

plt.figure(figsize=(20, 15))

# RF
plt.subplot(1, 3, 1)
ConfusionMatrixDisplay.from_predictions(y_test, y_pred_rf, cmap='Blues', ax=plt.gca())
plt.title("Confusion Matrix: Random Forest")

# XGB
plt.subplot(1, 3, 2)
ConfusionMatrixDisplay.from_predictions(y_test, y_pred_xgb, cmap='Greens', ax=plt.gca())
plt.title("Confusion Matrix: XGBoost")

# LR
plt.subplot(1, 3, 3)
ConfusionMatrixDisplay.from_predictions(y_test, y_pred_lr, cmap='Purples', ax=plt.gca())
plt.title("Confusion Matrix: Logistic Regression")

plt.tight_layout()
plt.show()

models = ['Random Forest', 'XGBoost', 'Logistic Regression']
accuracies = [
    accuracy_score(y_test, y_pred_rf),
    accuracy_score(y_test, y_pred_xgb),
    accuracy_score(y_test, y_pred_lr)
]
times = [cpu_train_time, xgb_train_time, lr_train_time]

# Accuracy Comparison
plt.figure(figsize=(10, 4))
plt.subplot(1, 2, 1)
sns.barplot(x=models, y=accuracies, palette='pastel')
plt.title("Model Accuracy Comparison")
plt.ylabel("Accuracy")

# Training Time Comparison
plt.subplot(1, 2, 2)
sns.barplot(x=models, y=times, palette='muted')
plt.title("Training Time Comparison")
plt.ylabel("Seconds")

plt.tight_layout()
plt.show()

summary_df = pd.DataFrame({
    "Model": models,
    "Accuracy": accuracies,
    "Training Time (s)": times
})
print(summary_df)



"""## Flood Prediction Model Comparison Report

This report summarizes the process and results of training and comparing three different machine learning models for flood prediction: Random Forest, XGBoost, and Logistic Regression.

### 1. Data Loading and Preparation

The dataset `synthetic_flood_dataset.csv` was loaded into a pandas DataFrame. The dataset has 10000 rows and 22 columns.

- The target variable `FLOOD` was separated from the features.
- The features were scaled using `StandardScaler`.
- The data was split into training and testing sets with a test size of 20%.

### 2. Model Training and Evaluation

Three models were trained and evaluated:

#### a) Random Forest

- A `RandomForestClassifier` with 60 estimators was trained.
- **Accuracy:** 0.9835
- **Training Time:** 1.43 seconds
- **Classification Report:**
"""
